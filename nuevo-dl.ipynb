{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2243895,"sourceType":"datasetVersion","datasetId":1347338},{"sourceId":8628530,"sourceType":"datasetVersion","datasetId":5166064},{"sourceId":181911146,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from PIL import Image, ImageOps\nimport torch.nn.functional as F\nimport torch\nfrom torch.utils.data import Dataset,DataLoader\nfrom torchvision import transforms\nfrom transformers import AutoImageProcessor, BertTokenizer, VisionEncoderDecoderModel,  ViTFeatureExtractor","metadata":{"execution":{"iopub.status.busy":"2024-06-06T21:34:25.183513Z","iopub.execute_input":"2024-06-06T21:34:25.183958Z","iopub.status.idle":"2024-06-06T21:34:25.189413Z","shell.execute_reply.started":"2024-06-06T21:34:25.183918Z","shell.execute_reply":"2024-06-06T21:34:25.188192Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import os\n\ntranspoop=transforms.PILToTensor()\n\ndef check_tensor_dimensions(tensor):\n    if tensor.dim() != 3:\n        return False\n\n    if tensor.size(0) != 3:\n        return False\n\n    if tensor.size(1) <= 1 or tensor.size(2) <= 1:\n        return False\n\n    return True\n\nDATA_INPUT_PATH = \"/kaggle/input/iam-handwriting-word-database\"\n\nimages_path = []\nlabels = []\n\ndef preprocess_dataset():\n    characters = set()\n    max_len = 0\n    with open(os.path.join(DATA_INPUT_PATH, 'iam_words', 'words.txt'), 'r') as file:\n        lines = file.readlines()\n\n        for line_number, line in enumerate(lines):\n            # Skip comments and empty lines\n            if line.startswith('#') or line.strip() == '':\n                continue\n\n            # Split the line and extract information\n            parts = line.strip().split()\n\n            # Continue with the rest of the code\n            word_id = parts[0]\n\n            first_folder = word_id.split(\"-\")[0]\n            second_folder = first_folder + '-' + word_id.split(\"-\")[1]\n\n            # Construct the image filename\n            image_filename = f\"{word_id}.png\"\n            image_path = os.path.join(\n                DATA_INPUT_PATH, 'iam_words', 'words', first_folder, second_folder, image_filename)\n\n            # Check if the image file exists\n            if os.path.isfile(image_path) and os.path.getsize(image_path):\n                \n                image2 = Image.open(image_path).convert(\"RGB\")\n                check = transpoop(image2)\n                # print(check.shape)\n                if check_tensor_dimensions(check)==True :\n                    images_path.append(image_path)\n                    # print(\"fuck me\")\n                    # Extract labels\n                    label = parts[-1].strip()\n                    for char in label:\n                        characters.add(char)\n\n                    max_len = max(max_len, len(label))\n                    labels.append(label)\n                    \n\n    characters = sorted(list(characters))\n\n    print('characters: ', characters)\n    print('max_len: ', max_len)\n    print(len(images_path))\n#     # Mapping characters to integers.\n#     char_to_num = tf.keras.layers.StringLookup(\n#         vocabulary=list(characters), mask_token=None)\n\n#     # Mapping integers back to original characters.\n#     num_to_char = tf.keras.layers.StringLookup(\n#         vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n#    )\n    return characters, max_len # char_to_num, num_to_char, \n    \ncharacters, max_len = preprocess_dataset()","metadata":{"execution":{"iopub.status.busy":"2024-06-06T21:34:25.191476Z","iopub.execute_input":"2024-06-06T21:34:25.191959Z","iopub.status.idle":"2024-06-06T21:35:43.277006Z","shell.execute_reply.started":"2024-06-06T21:34:25.191927Z","shell.execute_reply":"2024-06-06T21:35:43.276065Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"characters:  ['!', '\"', '#', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\nmax_len:  19\n44531\n","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install Pillow","metadata":{"execution":{"iopub.status.busy":"2024-06-06T21:35:43.279675Z","iopub.execute_input":"2024-06-06T21:35:43.279952Z","iopub.status.idle":"2024-06-06T21:35:43.284001Z","shell.execute_reply.started":"2024-06-06T21:35:43.279928Z","shell.execute_reply":"2024-06-06T21:35:43.283090Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-base-patch4-window7-224-in22k\")\n# image_processor =  ViTFeatureExtractor(image_mean=[0.5],image_std=[0.5])\n# image_processor =  ViTFeatureExtractor()\ntokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n\ntranspoop=transforms.PILToTensor()\n\ndef check_tensor_dimensions(tensor):\n    if tensor.dim() != 3:\n        return False, \"The tensor is not 3-dimensional.\"\n\n    if tensor.size(0) != 3:\n        return False, \"The first dimension is not 3.\"\n\n    if tensor.size(1) <= 1 or tensor.size(2) <= 1:\n        return False, \"The second or third dimension is not greater than 1.\"\n\n    return True, \"The tensor has the correct dimensions.\"\n\ndef resize_with_padding(image, target_size):\n    # Open an image file\n    # with Image.open(image_path) as img:\n    # Calculate the new size while maintaining the aspect ratio\n    image.thumbnail(target_size, Image.ANTIALIAS)\n        \n    # Create a new image with the target size and a white background\n    new_img = Image.new(\"RGB\", target_size, (255, 255, 255))\n        \n    # Calculate the position to paste the resized image on the new image\n    paste_position = ((target_size[0] - image.width) // 2,\n                          (target_size[1] - image.height) // 2)\n        \n    # Paste the resized image onto the new image\n    new_img.paste(image, paste_position)\n        \n    return new_img\n\ndef pad_to_size(tensor, desired_size=(1, 40), padding_value=0):\n    \n    current_size = tensor.size()\n    \n    # Check if the tensor already has the desired size\n    if current_size == desired_size:\n        return tensor\n    \n    # Calculate the padding needed\n    pad_length = desired_size[1] - current_size[1]\n    \n    # Ensure the tensor is 2-dimensional (1, original_length)\n    tensor = tensor.unsqueeze(0) if len(tensor.size()) == 1 else tensor\n    \n    # Apply padding\n    padded_tensor = F.pad(tensor, (0, pad_length), 'constant', padding_value)\n    \n    return padded_tensor\n    \nclass ImageTextDataset(Dataset):\n    def __init__(self, image_paths, labels,processor,tokenizer):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.processor = processor\n        self.tokenizer = tokenizer\n#         self.transform = transform\n#         self.target_transform = target_transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = Image.open(image_path).convert(\"RGB\")\n        label = self.labels[idx]\n        \n        \n        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n        label = self.tokenizer(label, padding=\"max_length\", max_length=128).input_ids\n        \n        label = [label1 if label1 != self.tokenizer.pad_token_id else -100 for label1 in label]\n\n        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(label)}\n        return encoding\n        \n        \n        # torch.tensor(label)\n        \n        # return image, label\n    \n# custom_transform = transforms.Compose([\n#     transforms.Lambda(lambda img: resize_with_padding(image=img, target_size=(224, 224))),\n#     transforms.Lambda(lambda img: image_processor(img, return_tensors=\"pt\").pixel_values)\n#     # transforms.ToTensor(),\n#     # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n# ])\n    \n# target_transform1=transforms.Compose([\n#     transforms.Lambda(lambda lbl: tokenizer.encode(lbl,return_tensors=\"pt\")),\n#     transforms.Lambda(lambda ten: pad_to_size(ten))\n# ])\ndataset = ImageTextDataset(image_paths=images_path, labels=labels, processor=image_processor, tokenizer=tokenizer) # , transform=custom_transform, target_transform=target_transform1)\n# dataloader = DataLoader(dataset, batch_size=12, shuffle=True, num_workers=4)\n\nprint(\"Number of training examples:\", len(dataset))\nfor i in range(len(dataset)):\n    check=dataset[i]\n\n# Example usage\n# resized_image = resize_with_padding(\"example.jpg\", (300, 300))\n# resized_image.show()\n# resized_image.save(\"resized_with_padding.jpg\")","metadata":{"execution":{"iopub.status.busy":"2024-06-06T21:35:43.286567Z","iopub.execute_input":"2024-06-06T21:35:43.286906Z","iopub.status.idle":"2024-06-06T21:38:43.834659Z","shell.execute_reply.started":"2024-06-06T21:35:43.286875Z","shell.execute_reply":"2024-06-06T21:38:43.833779Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Number of training examples: 44531\n","output_type":"stream"}]},{"cell_type":"code","source":"# tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n\n# chlabels = tokenizer.encode(\n#     \"an image of two cats chilling on a couch\",\n#     return_tensors=\"pt\",\n#     padding=True,\n#     max_length = 40\n# )\n# print(\"Labels shape:\", chlabels.shape)\n\nencoding = dataset[3]\nfor k,v in encoding.items():\n  print(k, v.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T21:38:43.835898Z","iopub.execute_input":"2024-06-06T21:38:43.836270Z","iopub.status.idle":"2024-06-06T21:38:43.847598Z","shell.execute_reply.started":"2024-06-06T21:38:43.836236Z","shell.execute_reply":"2024-06-06T21:38:43.846761Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"pixel_values torch.Size([3, 224, 224])\nlabels torch.Size([128])\n","output_type":"stream"}]},{"cell_type":"code","source":"# for batch_idx, (images, labels) in enumerate(dataloader):\n#     print(f\"Batch {batch_idx + 1}\")\n#     print(\"Images shape:\", images.shape)\n#     print(\"Labels shape:\", labels.shape)\n#     # print(\"Images:\", images)\n#     # print(\"Labels:\", labels)\n#     print(\"\\n\")\n    \n#     # Break after printing a few batches for inspection\n#     if batch_idx == 1:\n#         break","metadata":{"execution":{"iopub.status.busy":"2024-06-06T21:38:43.848613Z","iopub.execute_input":"2024-06-06T21:38:43.848876Z","iopub.status.idle":"2024-06-06T21:38:43.852913Z","shell.execute_reply.started":"2024-06-06T21:38:43.848853Z","shell.execute_reply":"2024-06-06T21:38:43.852246Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"image1 = Image.open(images_path[3]).convert(\"RGB\")\nimage1","metadata":{"execution":{"iopub.status.busy":"2024-06-06T21:38:43.853777Z","iopub.execute_input":"2024-06-06T21:38:43.854217Z","iopub.status.idle":"2024-06-06T21:38:43.866831Z","shell.execute_reply.started":"2024-06-06T21:38:43.854192Z","shell.execute_reply":"2024-06-06T21:38:43.865887Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<PIL.Image.Image image mode=RGB size=166x78>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"label1 = encoding['labels']\nlabel1[label1 == -100] = tokenizer.pad_token_id\nlabel_str = tokenizer.decode(label1, skip_special_tokens=True)\nprint(label_str)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T21:38:43.868221Z","iopub.execute_input":"2024-06-06T21:38:43.868568Z","iopub.status.idle":"2024-06-06T21:38:43.876717Z","shell.execute_reply.started":"2024-06-06T21:38:43.868536Z","shell.execute_reply":"2024-06-06T21:38:43.875873Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"stop\n","output_type":"stream"}]},{"cell_type":"code","source":"model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n    \"microsoft/swin-base-patch4-window7-224-in22k\", \"google-bert/bert-base-uncased\"\n)\nmodel.config.decoder_start_token_id = tokenizer.cls_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n# make sure vocab size is set correctly\nmodel.config.vocab_size = model.config.decoder.vocab_size\n\n# set beam search parameters\nmodel.config.eos_token_id = tokenizer.sep_token_id\nmodel.config.max_length = 64\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4","metadata":{"execution":{"iopub.status.busy":"2024-06-06T21:38:43.880387Z","iopub.execute_input":"2024-06-06T21:38:43.880629Z","iopub.status.idle":"2024-06-06T21:39:26.687033Z","shell.execute_reply.started":"2024-06-06T21:38:43.880608Z","shell.execute_reply":"2024-06-06T21:39:26.686218Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b8d7737b86f4e7abaf11e414d961889"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/437M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c789c877a0364d98b23cbfdd08369594"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0df361d20b7b4f5eba5918dd32a29e94"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertLMHeadModel were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    predict_with_generate=True,\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    fp16=True, \n    output_dir=\"/kaggle/working/\",\n    logging_steps=2,\n    save_steps=1000,\n    eval_steps=200,\n    save_total_limit=4,\n    load_best_model_at_end=True,\n    learning_rate = 3e-5,\n    weight_decay = 0.01,\n    warmup_steps = 500 \n)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T21:39:26.688173Z","iopub.execute_input":"2024-06-06T21:39:26.688460Z","iopub.status.idle":"2024-06-06T21:39:28.081086Z","shell.execute_reply.started":"2024-06-06T21:39:26.688436Z","shell.execute_reply":"2024-06-06T21:39:28.080105Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install jiwer","metadata":{"execution":{"iopub.status.busy":"2024-06-06T21:39:28.082382Z","iopub.execute_input":"2024-06-06T21:39:28.083131Z","iopub.status.idle":"2024-06-06T21:39:43.240584Z","shell.execute_reply.started":"2024-06-06T21:39:28.083101Z","shell.execute_reply":"2024-06-06T21:39:43.239505Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting jiwer\n  Downloading jiwer-3.0.4-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (8.1.7)\nCollecting rapidfuzz<4,>=3 (from jiwer)\n  Downloading rapidfuzz-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nDownloading jiwer-3.0.4-py3-none-any.whl (21 kB)\nDownloading rapidfuzz-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\nSuccessfully installed jiwer-3.0.4 rapidfuzz-3.9.3\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_metric\n\ncer_metric = load_metric(\"cer\")","metadata":{"execution":{"iopub.status.busy":"2024-06-06T21:39:43.242403Z","iopub.execute_input":"2024-06-06T21:39:43.243332Z","iopub.status.idle":"2024-06-06T21:39:44.418969Z","shell.execute_reply.started":"2024-06-06T21:39:43.243293Z","shell.execute_reply":"2024-06-06T21:39:44.418012Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/152175726.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  cer_metric = load_metric(\"cer\")\n/opt/conda/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for cer contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/cer/cer.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15333acda12741b584b9f23e0237ed41"}},"metadata":{}}]},{"cell_type":"code","source":"def compute_metrics(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n\n    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n\n    return {\"cer\": cer}","metadata":{"execution":{"iopub.status.busy":"2024-06-06T21:39:44.420161Z","iopub.execute_input":"2024-06-06T21:39:44.420463Z","iopub.status.idle":"2024-06-06T21:39:44.426111Z","shell.execute_reply.started":"2024-06-06T21:39:44.420437Z","shell.execute_reply":"2024-06-06T21:39:44.425264Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train_dataset, eval_dataset= torch.utils.data.random_split(dataset, (40078,4453 ))","metadata":{"execution":{"iopub.status.busy":"2024-06-06T21:39:44.427729Z","iopub.execute_input":"2024-06-06T21:39:44.428002Z","iopub.status.idle":"2024-06-06T21:39:44.442790Z","shell.execute_reply.started":"2024-06-06T21:39:44.427979Z","shell.execute_reply":"2024-06-06T21:39:44.441977Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"print(len(train_dataset),' ',len(eval_dataset))","metadata":{"execution":{"iopub.status.busy":"2024-06-06T21:39:44.443901Z","iopub.execute_input":"2024-06-06T21:39:44.444188Z","iopub.status.idle":"2024-06-06T21:39:44.449247Z","shell.execute_reply.started":"2024-06-06T21:39:44.444155Z","shell.execute_reply":"2024-06-06T21:39:44.448340Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"40078   4453\n","output_type":"stream"}]},{"cell_type":"code","source":" !pip install wandb --upgrade","metadata":{"execution":{"iopub.status.busy":"2024-06-06T21:39:44.450259Z","iopub.execute_input":"2024-06-06T21:39:44.450880Z","iopub.status.idle":"2024-06-06T21:39:56.960215Z","shell.execute_reply.started":"2024-06-06T21:39:44.450854Z","shell.execute_reply":"2024-06-06T21:39:56.959261Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.17.0)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.3.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"#from transformers import default_data_collator\nfrom transformers import DefaultDataCollator\nfrom transformers import  DataCollatorForSeq2Seq\nfrom transformers import ViTFeatureExtractor\n\n# default_data_collator= DataCollatorForSeq2Seq(tokenizer=tokenizer,model=model,padding='max_length',max_length=128)\ndefault_data_collator= DefaultDataCollator()\n# instantiate trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=default_data_collator,\n)\ntrainer.train() ","metadata":{"execution":{"iopub.status.busy":"2024-06-06T21:51:03.496103Z","iopub.execute_input":"2024-06-06T21:51:03.496472Z","iopub.status.idle":"2024-06-07T03:55:31.796144Z","shell.execute_reply.started":"2024-06-06T21:51:03.496441Z","shell.execute_reply":"2024-06-07T03:55:31.795054Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15030' max='15030' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15030/15030 6:04:25, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Cer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>2.407900</td>\n      <td>2.357545</td>\n      <td>0.806977</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.390300</td>\n      <td>2.312814</td>\n      <td>0.966826</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.812000</td>\n      <td>2.053372</td>\n      <td>0.773336</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>2.391900</td>\n      <td>1.980122</td>\n      <td>0.759007</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.668200</td>\n      <td>1.916861</td>\n      <td>0.778009</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.483100</td>\n      <td>1.847120</td>\n      <td>0.753245</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>2.364600</td>\n      <td>1.803495</td>\n      <td>0.700602</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.121000</td>\n      <td>1.728899</td>\n      <td>0.665559</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.807700</td>\n      <td>1.674873</td>\n      <td>0.662652</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>2.376600</td>\n      <td>1.590754</td>\n      <td>0.645104</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>1.294200</td>\n      <td>1.534294</td>\n      <td>0.626830</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>1.537600</td>\n      <td>1.570710</td>\n      <td>0.614215</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>1.456400</td>\n      <td>1.465695</td>\n      <td>0.608556</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>1.672500</td>\n      <td>1.440017</td>\n      <td>0.592047</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.809800</td>\n      <td>1.432236</td>\n      <td>0.561520</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.762600</td>\n      <td>1.384064</td>\n      <td>0.598847</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>1.641200</td>\n      <td>1.374865</td>\n      <td>0.561935</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>1.271100</td>\n      <td>1.333613</td>\n      <td>0.540079</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>2.021000</td>\n      <td>1.303227</td>\n      <td>0.521182</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.458000</td>\n      <td>1.279628</td>\n      <td>0.544284</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.912600</td>\n      <td>1.254158</td>\n      <td>0.529436</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>1.321500</td>\n      <td>1.238390</td>\n      <td>0.521908</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.983800</td>\n      <td>1.214446</td>\n      <td>0.505243</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.423200</td>\n      <td>1.204421</td>\n      <td>0.505815</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>1.722500</td>\n      <td>1.196261</td>\n      <td>0.480791</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.866800</td>\n      <td>1.165948</td>\n      <td>0.493822</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>1.223300</td>\n      <td>1.167975</td>\n      <td>0.472173</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.641500</td>\n      <td>1.174360</td>\n      <td>0.467345</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>0.776500</td>\n      <td>1.142986</td>\n      <td>0.481466</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.220300</td>\n      <td>1.128706</td>\n      <td>0.449849</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>0.804200</td>\n      <td>1.117748</td>\n      <td>0.458364</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>0.917100</td>\n      <td>1.109089</td>\n      <td>0.451511</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>0.981400</td>\n      <td>1.089179</td>\n      <td>0.440920</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>1.058700</td>\n      <td>1.084727</td>\n      <td>0.442529</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.903800</td>\n      <td>1.091882</td>\n      <td>0.435676</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>0.846200</td>\n      <td>1.071914</td>\n      <td>0.431939</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>0.334200</td>\n      <td>1.051430</td>\n      <td>0.416883</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>1.036100</td>\n      <td>1.048981</td>\n      <td>0.414391</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>0.749800</td>\n      <td>1.044889</td>\n      <td>0.415326</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.396800</td>\n      <td>1.037332</td>\n      <td>0.400218</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>1.379200</td>\n      <td>1.018229</td>\n      <td>0.393054</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>1.888500</td>\n      <td>1.012626</td>\n      <td>0.398453</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>1.496500</td>\n      <td>1.011165</td>\n      <td>0.400945</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>0.755400</td>\n      <td>1.006916</td>\n      <td>0.392846</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.935500</td>\n      <td>0.983827</td>\n      <td>0.386980</td>\n    </tr>\n    <tr>\n      <td>9200</td>\n      <td>0.765600</td>\n      <td>0.972516</td>\n      <td>0.374208</td>\n    </tr>\n    <tr>\n      <td>9400</td>\n      <td>1.323400</td>\n      <td>0.969315</td>\n      <td>0.367719</td>\n    </tr>\n    <tr>\n      <td>9600</td>\n      <td>0.510600</td>\n      <td>0.956042</td>\n      <td>0.362268</td>\n    </tr>\n    <tr>\n      <td>9800</td>\n      <td>1.019000</td>\n      <td>0.932623</td>\n      <td>0.360503</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>1.275700</td>\n      <td>0.937467</td>\n      <td>0.365850</td>\n    </tr>\n    <tr>\n      <td>10200</td>\n      <td>0.395900</td>\n      <td>0.946883</td>\n      <td>0.354169</td>\n    </tr>\n    <tr>\n      <td>10400</td>\n      <td>0.381100</td>\n      <td>0.948005</td>\n      <td>0.362994</td>\n    </tr>\n    <tr>\n      <td>10600</td>\n      <td>0.409500</td>\n      <td>0.946235</td>\n      <td>0.365227</td>\n    </tr>\n    <tr>\n      <td>10800</td>\n      <td>0.617900</td>\n      <td>0.937780</td>\n      <td>0.354480</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.345700</td>\n      <td>0.933463</td>\n      <td>0.347939</td>\n    </tr>\n    <tr>\n      <td>11200</td>\n      <td>0.543600</td>\n      <td>0.927558</td>\n      <td>0.338854</td>\n    </tr>\n    <tr>\n      <td>11400</td>\n      <td>0.485000</td>\n      <td>0.920659</td>\n      <td>0.339840</td>\n    </tr>\n    <tr>\n      <td>11600</td>\n      <td>0.401500</td>\n      <td>0.917204</td>\n      <td>0.339217</td>\n    </tr>\n    <tr>\n      <td>11800</td>\n      <td>0.229600</td>\n      <td>0.916451</td>\n      <td>0.339269</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.513500</td>\n      <td>0.919265</td>\n      <td>0.343059</td>\n    </tr>\n    <tr>\n      <td>12200</td>\n      <td>0.391100</td>\n      <td>0.916077</td>\n      <td>0.335583</td>\n    </tr>\n    <tr>\n      <td>12400</td>\n      <td>0.729600</td>\n      <td>0.914810</td>\n      <td>0.327277</td>\n    </tr>\n    <tr>\n      <td>12600</td>\n      <td>0.377000</td>\n      <td>0.904618</td>\n      <td>0.328990</td>\n    </tr>\n    <tr>\n      <td>12800</td>\n      <td>0.766800</td>\n      <td>0.903560</td>\n      <td>0.325096</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.509500</td>\n      <td>0.902044</td>\n      <td>0.331378</td>\n    </tr>\n    <tr>\n      <td>13200</td>\n      <td>0.459800</td>\n      <td>0.891997</td>\n      <td>0.328990</td>\n    </tr>\n    <tr>\n      <td>13400</td>\n      <td>0.825300</td>\n      <td>0.891557</td>\n      <td>0.325356</td>\n    </tr>\n    <tr>\n      <td>13600</td>\n      <td>0.661000</td>\n      <td>0.884607</td>\n      <td>0.325771</td>\n    </tr>\n    <tr>\n      <td>13800</td>\n      <td>0.807500</td>\n      <td>0.885157</td>\n      <td>0.321722</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.479100</td>\n      <td>0.882845</td>\n      <td>0.322293</td>\n    </tr>\n    <tr>\n      <td>14200</td>\n      <td>0.689000</td>\n      <td>0.879158</td>\n      <td>0.318191</td>\n    </tr>\n    <tr>\n      <td>14400</td>\n      <td>0.771400</td>\n      <td>0.878221</td>\n      <td>0.315284</td>\n    </tr>\n    <tr>\n      <td>14600</td>\n      <td>0.606300</td>\n      <td>0.879317</td>\n      <td>0.314298</td>\n    </tr>\n    <tr>\n      <td>14800</td>\n      <td>0.867800</td>\n      <td>0.878445</td>\n      <td>0.314972</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.691000</td>\n      <td>0.877181</td>\n      <td>0.315076</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nThere were missing keys in the checkpoint model loaded: ['decoder.cls.predictions.decoder.weight', 'decoder.cls.predictions.decoder.bias'].\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=15030, training_loss=1.0450003768922793, metrics={'train_runtime': 21867.7554, 'train_samples_per_second': 5.498, 'train_steps_per_second': 0.687, 'total_flos': 2.188943199833061e+19, 'train_loss': 1.0450003768922793, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import GenerationConfig\nimport requests\nmodel = VisionEncoderDecoderModel.from_pretrained(\"/kaggle/working/checkpoint-15000\")\ngeneration_config = GenerationConfig.from_pretrained(\"/tmp\", \"/kaggle/working/checkpoint-15000/generation_config.json\")\ntokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\nimage_processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-base-patch4-window7-224-in22k\")\n# url = \"https://b2633864.smushcdn.com/2633864/wp-content/uploads/2020/08/ocr_handwriting_reco_adrian_sample.jpg?lossy=2&strip=1&webp=1\"\nimage = Image.open(\"/kaggle/input/zauraiz/zauraiz.jpg\").convert(\"RGB\")\npixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\ngenerated_ids = model.generate(pixel_values)\ngenerated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T05:21:46.240041Z","iopub.execute_input":"2024-06-07T05:21:46.240520Z","iopub.status.idle":"2024-06-07T05:21:53.106431Z","shell.execute_reply.started":"2024-06-07T05:21:46.240480Z","shell.execute_reply":"2024-06-07T05:21:53.105363Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"hebrew\n","output_type":"stream"}]},{"cell_type":"code","source":"# image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-base-patch4-window7-224-in22k\")\n# tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n# dataset = load_dataset(\"huggingface/cats-image\")\n# image = dataset[\"test\"][\"image\"][0]\n# pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n\n# labels = tokenizer(\n#     \"an image of two cats chilling on a couch\",\n#     return_tensors=\"pt\",\n# ).input_ids\n\n# model.config.decoder_start_token_id = tokenizer.cls_token_id\n# model.config.pad_token_id = tokenizer.pad_token_id\n\n# # the forward function automatically creates the correct decoder_input_ids\n# loss = model(pixel_values=pixel_values, labels=labels).loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}